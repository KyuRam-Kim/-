{"cells":[{"cell_type":"markdown","metadata":{"id":"skrui3ca_6XD"},"source":["# <h1> 3-1 K-최근접 이웃 회귀 </h1>\n","\n","* 지도학습 알고리즘 \n","\n","  -> 분류 : 샘플을 몇 개의 클래스 중 하나로 분류하는 문제\n","\n","  -> ***회귀(Regression) : 임의의 어떤 숫자를 예측하는 문제***\n","\n","\n","---\n","\n","\n","<h3> 농어의 무게 예측하기 </h3>\n","\n","\n","*  맷플롯립, 넘파이\n","*  사이킷런 (KNeighborsRegressor : K-최근접 이웃 회귀 알고리즘에 사용되는 클래스)\n","\n","---\n","\n","1. 결정계수 (R<sup>2</sup> , Coefficient of Determination)\n","\n","2. 과대적합 & 과소적합 (Overfitting & Underfitting)\n","  * 훈련세트의 결정계수 점수와 테스트세트의 결정계수 점수 비교( knr.score() 사용 )\n","  * 과대적합 : 훈련세트 점수는 좋았는데 테스트세트의 점수가 굉장히 나쁜 경우\n","    - 모델을 덜 복잡하게 수정하자\n","    - K-최근접 이웃 회귀에서는 이웃의 갯수를 늘림\n","  * 과소적합 : 테스트세트의 점수가 훈련세트 점수보다 높거나. 두 점수 모두 굉장히 낮은 경우 \n","    - 모델을 더 복잡하게 수정하자\n","    - K-최근접 이웃 회귀에서는 이웃의 갯수를 줄임\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntnZmRym__Oc"},"outputs":[],"source":["#############################################################################\n","# K-최근접 이웃 회귀\n","\n","# 데이터 준비\n","import numpy as np\n","perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,\n","       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,\n","       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,\n","       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,\n","       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,\n","       44.0])     #농어의 길이\n","perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,\n","       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,\n","       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,\n","       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,\n","       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,\n","       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,\n","       1000.0])   #농어의 무게\n","\n","#산점도 그리기\n","import matplotlib.pyplot as plt\n","plt.scatter(perch_length, perch_weight)\n","plt.xlabel(\"Length\")\n","plt.ylabel(\"Weight\")\n","plt.show()\n","\n","#농어 데이터를 훈련세트/테스트세트로 나누기\n","#길이를 이용하여 무게를 예측하려고 함\n","#     -> 농어의 길이: 특성 , 농어의 무게 : 타깃\n","from sklearn.model_selection import train_test_split\n","train_input, test_input, train_target, test_target = train_test_split(\n","    perch_length, perch_weight, random_state=42)\n","\n","#사이킷런에 사용할 훈련세트는 2차원 배열이여야 함.\n","#방금 생성된 1차원 배열들을 2차원 배열로 바꾸자\n","\n","#넘파이 패키지에서 reshape() 메서드 이용\n","#예시\n","test = np.array([1,2,3,4])\n","print(test.shape)           #(4,) 출력\n","test = test.reshape(2,2)\n","print(test)                 # 2 x 2인 2차원 배열로 바뀐걸 확인 가능\n","print(test.shape)           #(2, 2) 출력\n","#reshape() 메서드 이용시 지정한 크기가 원본 배열의 원소의 갯수와 다르면 안됨\n","\n","train_input = train_input.reshape(-1,1)      # -1은 나머지 원소 갯수로 모두 채우라는 의미\n","test_input = test_input.reshape(-1,1)        # 원소 갯수를 매번 외워도 되지 않아서 편리함\n","print(train_input.shape, test_input.shape)   # 2차원 배열로 바뀐 것을 확인 가능\n","\n","#############################################################################\n","#결정계수\n","from sklearn.neighbors import KNeighborsRegressor\n","knr = KNeighborsRegressor()\n","\n","knr.fit(train_input,train_target)               #K-최근접 이웃 회귀 모델을 훈련\n","print(knr.score(test_input, test_target))       #분류의 경우는 정답을 맞힌 갯수의 비율이지만,\n","                                                #회귀에서는 결정계수 이용\n","\n","#타깃과 예측한 값 사이의 차이를 구하여 예측이 어느 정도 벗어났는지 알아보자\n","# sklearn.metrics 모듈 이용\n","from sklearn.metrics import mean_absolute_error\n","\n","test_prediction = knr.predict(test_input)     #테스트 세트에 대한 예측을 만들자\n","\n","mae = mean_absolute_error(test_target, test_prediction)   #테스트 세트에 대한 평균 절댓값 오차를 계산\n","print(mae)    #약 19.2, 예측값은 평균적으로 19g 정도 타깃값과 다르다는 뜻\n","\n","#############################################################################\n","#과대적합 vs 과소적합\n","\n","#훈련한 모델을 사용해 훈련세트의 결정계수 점수를 구해보자\n","print(knr.score(train_input, train_target))\n","\n","#과대적합 : 훈련세트에서 점수가 좋았는데 테스트 세트에서 점수가 굉장히 나쁜 경우\n","#         : 즉, 훈련세트에만 잘 맞는 모델, 실전에 투입 어려움\n","#과소적합 : 훈련세트보다 테스트 세트의 점수가 높거나, 두 점수가 모두 너무 낮은 경우 \n","#         : 모델이 너무 단순하여 훈련세트에 적절히 훈련되지 않은 경우이다\n","#         : 훈련세트, 테스트세트의 크기가 매우 작을 경우에도 발생함\n","\n","# 현재의 경우는 과소적합 (훈련세트 결정계수: 0.9698 , 테스트세트 결정계수: 0.9928)\n","# 따라서 모델을 더 복잡하게 만들자 (= 이웃의 갯수를 줄이자)\n","\n","knr.n_neighbors = 3                     #이웃의 갯수를 3으로 수정\n","knr.fit(train_input, train_target)      #모델을 다시 훈련합니다\n","\n","#결정계수 다시 확인\n","print(knr.score(train_input, train_target))   #0.98049 출력\n","print(knr.score(test_input, test_target))     #0.97465 출력\n","#훈련세트의 결정계수가 더 커졌으며, 둘의 차이도 크지 않기에 과소적합 문제 해결됨\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EBe3iTmHc1JD"},"source":["교재 연습문제"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RD3lun3lcyy7"},"outputs":[],"source":["#연습문제 p128\n","\n","import numpy as np\n","perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,\n","       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,\n","       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,\n","       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,\n","       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,\n","       44.0])     #농어의 길이\n","perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,\n","       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,\n","       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,\n","       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,\n","       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,\n","       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,\n","       1000.0])   #농어의 무게\n","\n","from sklearn.model_selection import train_test_split\n","train_input, test_input, train_target, test_target = train_test_split(\n","    perch_length, perch_weight, random_state=42)\n","\n","train_input = train_input.reshape(-1,1)      \n","test_input = test_input.reshape(-1,1)        \n","\n","from sklearn.neighbors import KNeighborsRegressor\n","knr = KNeighborsRegressor()             # K-최근접 이웃 회귀의 객체를 만듭니다\n","\n","x = np.arange(5,45).reshape(-1,1)       # 5에서 45까지 농어의 길이를 만들어 2차원 배열로 바꿈\n","\n","import matplotlib.pyplot as plt\n","for n in [1,5,10]:                      # 이웃 갯수가 1, 5, 10일때 예측결과를 그래프로 그려보자\n","  knr.n_neighbors = n\n","  knr.fit(train_input, train_target)\n","  prediction =  knr.predict(x)          # 지정한 범위 x에 대한 예측을 구합니다\n","\n","  plt.scatter(train_input, train_target)\n","  plt.plot(x, prediction)\n","  plt.title(\"n_neighbors = %d\" %n)\n","  plt.xlabel(\"Length\")\n","  plt.ylabel(\"Weight\")\n","  plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"u9oxGwDn_0L-"},"source":["**===================================================================================================**"]},{"cell_type":"markdown","metadata":{"id":"RN2teltjbzqu"},"source":["# <h1> 3-2 선형 회귀 (Linear Regression) </h1>\n","\n","*  K- 최근접 이웃 회귀의 한계\n","\n","  -> 새로운 샘플이 훈련세트의 범위를 벗어나면 엉뚱한 값을 예측할 수 있음\n","\n","* 선형 회귀 (Linear Regression) : 하나의 특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾음\n","\n","* 다항 회귀 (Polynomial Regression) : 다항식을 이용하여 특성과 타깃 사이의 관계를 나타냄. 함수는 비선형일 수 있지만 여전히 선형 회귀로 표현 가능\n","\n","---\n","\n","\n","<h3> 농어의 무게 예측하기 </h3>\n","\n","\n","*  맷플롯립, 넘파이\n","*  사이킷런 (sklearn.linear_model 모듈 아래 LinearRegression 클래스) \n","\n","---\n","\n","1. 모델 파라미터 (Model Parameter) : \n","\n","  머신러닝 알고리즘이 찾은 값 (ex. coef_ , intercept_ )\n","\n","  많은 머신러닝 알고리즘의 훈련 과정은 최적의 모델 파라미터를 찾는 것\n","\n","2. 다항 회귀에서 변수의 거듭제곱을 다른 변수로 치환하여 생각하자\n","\n","  본 예시에서 나온 2차 방정식에서 : 무게는 길이<sup>2</sup> 과 길이의 선형관계로 표현할 수 있다\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmTWm5vScE40"},"outputs":[],"source":["#############################################################################\n","#선형회귀\n","\n","#K-최근접 이웃의 한계\n","import numpy as np\n","perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,\n","       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,\n","       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,\n","       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,\n","       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,\n","       44.0])     #농어의 길이\n","perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,\n","       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,\n","       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,\n","       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,\n","       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,\n","       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,\n","       1000.0])   #농어의 무게\n","\n","from sklearn.model_selection import train_test_split\n","\n","#훈련세트와 테스트 세트로 나눕니다\n","train_input, test_input, train_target, test_target = train_test_split(\n","    perch_length, perch_weight, random_state = 42)\n","\n","#훈련세트와 테스트 세트를 2차원 배열로 바꿉니다\n","train_input = train_input.reshape(-1,1)\n","test_input = test_input.reshape(-1,1)\n","\n","from sklearn.neighbors import KNeighborsRegressor\n","knr=KNeighborsRegressor()\n","knr.n_neighbors = 3                     #이웃갯수는 3개\n","\n","#K-최근접 이웃 회귀 모델을 훈련합니다\n","knr.fit(train_input,train_target)\n","print(knr.predict([[50]]))              #1033.33g으로 예측\n","\n","import matplotlib.pyplot as plt\n","\n","#50cm 농어의 이웃을 구합니다\n","distances, indexes = knr.kneighbors([[50]])\n","#훈련세트와 50cm 농어, 이웃세트의 산점도를 그립니다\n","plt.scatter(train_input,train_target)\n","plt.scatter(50,knr.predict([[50]]), marker=\"^\")\n","plt.scatter(train_input[indexes],train_target[indexes], marker=\"D\")\n","plt.xlabel(\"Length\")\n","plt.ylabel(\"Weight\")\n","plt.show()\n","\n","#이웃들의 평균과 예측값을 비교해보자\n","print(np.mean(train_target[indexes]))\n","\n","#새로운 샘플이 훈련세트의 범위를벗어나면 엉뚱한 값을 예측할 수 있다\n","print(knr.predict([[100]]))       #100cm 짜리 농어도 1033.33g 으로 예측\n","\n","distances, indexes = knr.kneighbors([[100]])\n","plt.scatter(train_input,train_target)\n","plt.scatter(100,knr.predict([[100]]), marker=\"^\")\n","plt.scatter(train_input[indexes],train_target[indexes], marker=\"D\")\n","plt.xlabel(\"Length\")\n","plt.ylabel(\"Weight\")\n","plt.show()                       #산점도로 100cm 농어의 예측 과정 확인\n","\n","#############################################################################\n","\n","#선형 회귀\n","\n","from sklearn.linear_model import LinearRegression\n","lr = LinearRegression()\n","\n","lr.fit(train_input,train_target)        #선형 회귀 모델 훈련\n","print(lr.predict([[50]]))               #50cm 농어에 대해 예측 , 1241.83cm으로 예측\n","\n","#모델이 찾은 선형 회귀선의 기울기와 y절편 확인하기\n","print(lr.coef_)                         #기울기, 머신러닝에서는 기울기를 계수(coefficient) 또는 기울기라고 부름(weight)\n","print(lr.intercept_)                    \n","#coef_ 와 intercept_ 는 머신러닝 알고리즘이 찾은 값이라는 의미로 모델 파라미터(model parameter)라고 부름\n","\n","#회귀선을 훈련세트의 산점도와 함께 나타내어 보자 (농어길이 15~50)\n","plt.scatter(train_input,train_target)         # 훈련세트 산점도\n","plt.plot([15, 50], [15*lr.coef_+lr.intercept_ , 50*lr.coef_+lr.intercept_])       #회귀선\n","plt.scatter(50, lr.predict([[50]]), marker=\"^\")           # 50cm 농어 데이터\n","plt.xlabel(\"Length\")\n","plt.ylabel(\"Weight\")\n","plt.show()\n","\n","#훈련세트와 테스트세트의 결정계수 구하기\n","print(lr.score(train_input,train_target))   #훈련세트 : 0.9398463\n","print(lr.score(test_input,test_target))     #테스트세트 : 0.8247503\n","#전체적으로 과소적합\n","\n","#############################################################################\n","\n","#다항 회귀\n","#농어 데이터에서 선형 회귀선은 적합하지 않음. 2차 곡선을 구해보자\n","\n","#기존 농어의 길이를 제곱하여 원래 데이터 앞에 붙여보자\n","train_poly = np.column_stack((train_input**2, train_input))\n","test_poly = np.column_stack((test_input**2, test_input))\n","print(train_poly.shape, test_poly.shape)          #shape 파악 : (42,2) , (14,2)\n","\n","#새로 만든 훈련세트로 선형회귀를 훈련시켜보자\n","lr = LinearRegression()\n","lr.fit(train_poly, train_target)\n","print(lr.predict([[50**2,50]]))                   #1573.98 예측\n","\n","print(lr.coef_ , lr.intercept_)                   # [1.01433211 -21.55792498] 116.0502107827827 출력\n","#즉, 무게 = 1.01*길이^2 -21.6*길이 + 116.05\n","\n","#예측한 2차 회귀선을 그려보자\n","point = np.arange(15,51)                         #15부터 50까지 정수배열을 만들자\n","plt.scatter(train_input,train_target)\n","plt.plot(point, 1.01*point**2 - 21.6*point +116.05 )\n","plt.scatter(50, 1.01*50**2 - 21.6*50 +116.05, marker=\"^\")\n","plt.xlabel(\"Length\")\n","plt.ylabel(\"Weight\")\n","plt.show()\n","\n","#훈련세트와 테스트세트의 결정계수 구하기\n","print(lr.score(train_poly,train_target))   #훈련세트 :  0.9706807\n","print(lr.score(test_poly,test_target))     #테스트세트 : 0.9775935\n","#약간 과소적합\n","\n","#############################################################################\n","\n","#선형 회귀로 훈련 세트 범위 밖의 샘플 예측"]},{"cell_type":"markdown","source":["**===================================================================================================**"],"metadata":{"id":"Lcoa_y8gvzef"}},{"cell_type":"markdown","source":["# <h1> 3-3 특성 공학과 규제 (Feature Engineering and Regularization)</h1>\n","\n","*  다중 회귀 (Multiple Regression)\n","\n","* 특성 공학 (Feature Engineering) : 기존의 특성을 사용하여 새로운 특성을 뽑아내는 작업\n","\n","* 규제 (Regularization) : 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것\n","\n","  -> 선형 회귀 모델의 경우 특성에 곱해지는 계수(기울기)의 크기를 줄이는 작업\n","\n","* 릿지 회귀 (Ridge Regression) : 계수를 제곱한 값을 기준으로 규제를 적용\n","\n","  -> 라쏘 보다 릿지가 더 선호되는 경향이 있음 \n","\n","* 라쏘 회귀 (Lasso Regression) : 계수의 절댓값을 기준으로 규제를 적용\n","\n","  -> 라쏘는 계수의 값을 아예 0으로 만들 수 도 있음\n","\n","* 하이퍼 파라미터 (Hyperparameter) : 머신러닝 알고리즘이 학습하지 않는 파라미터. 사람이 사전에 지정해야함\n","\n","  -> 릿지와 라쏘의 Alpha 파라미터가 대표적\n","\n","\n","---\n","\n","\n","<h3> 농어의 무게 예측하기 </h3>\n","\n","\n","*  맷플롯립, 넘파이\n","*  사이킷런 \n","\n","  -> sklearn.preprocessing 모듈 아래의 PolynomialFeatures , StandardScaler 클래스\n","\n","  -> sklearn.linear_model 모듈 아래의 Ridge, Lasso 클래스\n","*  판다스 (pandas / 데이터프레임 구조)\n","\n","---\n","\n","1. 판다스의 데이터프레임은 판다스의 핵심 데이터 구조\n","\n","2. 과대적합한 모델은 규제를 걸어 훈련세트를 너무 과도하게 학습하지 못하도록 훼방을 놓자 \n","\n","3. 릿지와 라쏘는 규제가 걸린 선형회귀모델, 훈련세트/테스트세트 간의 결정계수 격차가 제일 좁은 alpha 값을 이용하자\n","    \n","     ->산점도 그려서 확인해보기"],"metadata":{"id":"ZBK7b9wQ_gP_"}},{"cell_type":"code","source":["#############################################################################\n","#특성공학과 규제\n","\n","#다중 회귀\n","#특성 공학 : 기존의 특성을 사용하여 새로운 특성을 뽑아내는 작업\n","\n","#############################################################################\n","#데이터 준비\n","import pandas as pd                               #판다스 임포트, 관례적으로 pd로 임포트\n","df = pd.read_csv(\"http://bit.ly/perch_csv_data\")  #csv파일 데이터프레임에 저장\n","perch_full = df.to_numpy()                        #넘파이 배열로 형태 변경\n","print(perch_full)                                 #데이터 확인\n","\n","import numpy as np\n","perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,\n","       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,\n","       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,\n","       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,\n","       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,\n","       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,\n","       1000.0])                                  #타겟 데이터 그대로 사용(농어의 무게)\n","\n","from sklearn.model_selection import train_test_split      #perch_full 과 perch_weight 를 훈련세트/테스트세트로 나누자\n","train_input, test_input, train_target, test_target = train_test_split(\n","    perch_full, perch_weight, random_state = 42)\n","\n","######################################################################################################\n","#사이킷런의 변환기\n","from sklearn.preprocessing import PolynomialFeatures      #특성을 만들거나 전처리하기 위한 클래스\n","poly = PolynomialFeatures()\n","\n","#예시 --------------------------------------------------------------------------------------------\n","#2개의 특성 2와 3으로 이루어진 샘플 하나 적용\n","poly.fit([[2,3]])                 # fit() : 새롭게 만들 특성 조합을 찾음       \n","print(poly.transform([[2,3]]))    # transform() : 실제로 데이터를 변환함, [1. 2. 3. 4. 6. 9] 반환\n","#기본적으로 각 특성을 제곱한 항을 추가하고 특성끼리 곱한 항을 추가함\n","#1은 왜 나왔을까? -> 선형 방정식의 절편은 항상 값이 1인 특성과 곱해지는 계수라고 생각할 수 있음\n","#                 -> 절편을 만들어내기 위한 특성임.\n","#그러나 사이킷런의 선형 모델은 자동으로 절편을 추가하므로 굳이 이러한 특성을 만들 필요는 없음\n","\n","#1을 제거해보자\n","poly = PolynomialFeatures(include_bias=False)   #include_bias 매개변수 False로\n","poly.fit([[2,3]])\n","print(poly.transform([[2,3]]))\n","# ------------------------------------------------------------------------------------------------\n","\n","# 이 방식으로 train_input에 적용시켜보자\n","poly = PolynomialFeatures(include_bias=False)\n","poly.fit(train_input)\n","train_poly = poly.transform(train_input)       #훈련세트 변환\n","print(train_poly.shape)                        #배열의 크기 확인      (42,9) 출력 : 42개의 샘플, 9개의 특성\n","poly.get_feature_names_out()                   #특성들의 생성 과정 확인\n","\n","test_poly = poly.transform(test_input)         #테스트세트 변환 (훈련세트로 학습한 변환기를 이용해 테스트세트까지 변환하는 습관을 들이자)\n","\n","######################################################################################################\n","#다중 회귀 모델 훈련하기\n","from sklearn.linear_model import LinearRegression\n","lr = LinearRegression()\n","lr.fit(train_poly,train_target)\n","print(lr.score(train_poly,train_target))      #훈련세트 결정계수 : 0.99031 출력\n","print(lr.score(test_poly,test_target))        #테스트세트 결정계수 : 0.97145 출력\n","\n","#특성을 더 많이 늘려보자\n","poly = PolynomialFeatures(degree = 5, include_bias=False)   #고차항의 최대 차수를 5제곱까지 만들어보자\n","poly.fit(train_input)\n","train_poly = poly.transform(train_input)\n","test_poly = poly.transform(test_input)\n","print(train_poly.shape)                       #(42,55) 출력, 특성의 갯수가 55개로 늘어남\n","\n","lr.fit(train_poly, train_target)\n","print(lr.score(train_poly,train_target))      #훈련세트 결정계수 : 0.99999 출력\n","print(lr.score(test_poly,test_target))        #테스트세트 결정계수 : -144.4 출력\n","#특성의 갯수를 크게 늘리면 선형 모델은 아주 강력해짐\n","#훈련세트에 너무 과대적합됨\n","\n","######################################################################################################\n","#규제 (Regularization)\n","from sklearn.preprocessing import StandardScaler\n","ss = StandardScaler()\n","ss.fit(train_poly)\n","train_scaled = ss.transform(train_poly)   #훈련세트 표준화\n","test_scaled = ss.transform(test_poly)     #테스트세트 표준화\n","\n","######################################################################################################\n","#릿지 회귀 (Ridge)\n","#계수를 제곱한 값을 기준으로 규제를 적용\n","from sklearn.linear_model import Ridge\n","ridge = Ridge()\n","ridge.fit(train_scaled, train_target)\n","print(ridge.score(train_scaled, train_target))    #훈련세트 평가 : 0.98961 출력\n","print(ridge.score(test_scaled, test_target))      #테스트세트 평가: : 0.97906 출력\n","\n","#규제의 양을 임의로 조절 가능\n","#alpha 값이 크면 규제 강도가 세짐 (계수값을 더 줄이고 더 과소적합 되도록 유도함)\n","#alpha 값이 작으면 규제 강도가 느슨해짐 (계수값을 줄이는 역할이 줄어들며 과대적합이 될 가능성이 큼)\n","#하이퍼 파라미터 (Hyperparameter) : alpha 값처럼 머신러닝 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터\n","\n","#적절한 alpha 값을 찾는 방법 : alpha 값에 대해 결정계수 값의 그래프를 그려 보는 것\n","#훈련세트와 테스트세트의 점수가 가장 가까운 지점이 최적의 alpha 값이 됨\n","import matplotlib.pyplot as plt\n","train_score = []\n","test_score = []\n","\n","alpha_list = [0.001,0.01,0.1,1,10,100]\n","for alpha in alpha_list:\n","  ridge = Ridge(alpha = alpha)          #릿지 모델을 만듬\n","  ridge.fit(train_scaled,train_target)  #릿지 모델 훈련\n","  train_score.append(ridge.score(train_scaled, train_target)) #훈련세트의 결정계수\n","  test_score.append(ridge.score(test_scaled, test_target))    #테스트세트의 결정계수\n","\n","#산점도를 그릴때 x축(알파)의 간격을 일정하게 하기 위하여 상용로그 사용\n","plt.plot(np.log10(alpha_list), train_score)\n","plt.plot(np.log10(alpha_list), test_score)\n","plt.xlabel(\"Alpha\")\n","plt.ylabel(\"R^2 Score\")\n","plt.show()\n","#x가 -1일때 (alpha가 0.1)일때 격차가 제일 작음\n","\n","#구한 alpha 값으로 최종 모델을 훈련시키자\n","ridge = Ridge(alpha=0.1)\n","ridge.fit(train_scaled,train_target)\n","print(ridge.score(train_scaled,train_target))     #훈련세트 0.99038 출력\n","print(ridge.score(test_scaled,test_target))       #테스트세트 0.98279 출력\n","\n","######################################################################################################\n","#라쏘 회귀 (Lasso)\n","#계수의 절댓값을 기준으로 규제를 적용, 방법은 릿지와 매우 비슷\n","from sklearn.linear_model import Lasso\n","lasso = Lasso()\n","lasso.fit(train_scaled,train_target)             #라쏘 회귀 훈련\n","print(lasso.score(train_scaled,train_target))    #훈련세트 결정계수 : 0.98978 출력\n","print(lasso.score(test_scaled,test_target))      #테스트세트 결정계수 : 0.98005 출력\n","\n","#산점도를 이용하여 적절한 alpha 값을 구해보자\n","train_score = []\n","test_score = []\n","alpha_list = [0.001,0.01,0.1,1,10,100]\n","for a in alpha_list:\n","  lasso = Lasso(alpha=a)\n","  lasso.fit(train_scaled,train_target)\n","  train_score.append(lasso.score(train_scaled,train_target))\n","  test_score.append(lasso.score(test_scaled,test_target))\n","plt.plot(np.log10(alpha_list), train_score)\n","plt.plot(np.log10(alpha_list), test_score)\n","plt.xlabel(\"Alpha\")\n","plt.ylabel(\"R^2 Score\")\n","plt.show() \n","#x가 1일때 (alpha가 10일 때) 격차가 제일 적음\n","\n","#구한 alpha 값으로 최종 모델을 훈련시키자\n","lasso = Lasso(alpha=10)\n","lasso.fit(train_scaled,train_target)\n","print(lasso.score(train_scaled,train_target))      #훈련세트 결정계수 : 0.988806 출력\n","print(lasso.score(test_scaled,test_target))        #테스트세트 결정계수 : 0.982447 출력\n","\n","#라쏘 모델의 계수 중 0인 계수를 헤아려 보자\n","print(np.sum(lasso.coef_== 0))                     # 40 출력\n","#np.sum 메서드 : 넘파이 배열에 비교연산자를 사용했을때 True를 1로, False를 0으로 인식하여 더함\n","#55개의 특셩중 40개 특성의 계수가 0 -> 라쏘 모델은 실제로 15개의 특성만 사용함\n","\n","######################################################################################################\n","#모델의 과대적합을 제어하기\n"],"metadata":{"id":"opqTgtmj_geX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**===================================================================================================**"],"metadata":{"id":"thI7wJa_v0kJ"}}],"metadata":{"colab":{"collapsed_sections":[],"name":"3. 회귀 알고리즘과 모델 규제 (Perch Regression).ipynb","provenance":[],"history_visible":true,"authorship_tag":"ABX9TyPLy4UiE5DkDBxhEELuh1ai"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}